{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76629c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0bce03",
   "metadata": {},
   "source": [
    "code-neuron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12891f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, n_inputs):\n",
    "        # small random weights\n",
    "        self.w = np.random.randn(n_inputs) * 0.01\n",
    "        self.b = 0.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (n_inputs,)\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9ae12d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: 0.021043225740068748\n"
     ]
    }
   ],
   "source": [
    "n = Neuron(n_inputs=3)\n",
    "x = np.array([1.0, 2.0, -1.0])\n",
    "print(\"Output:\", n.forward(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cff1c9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0931ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, n_inputs):\n",
    "        self.w = np.random.randn(n_inputs) * 0.01\n",
    "        self.b = 0.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        a = relu(z)\n",
    "        return a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c3c27",
   "metadata": {},
   "source": [
    "dense-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2bb2845",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.W = np.random.randn(n_inputs, n_neurons) * 0.01\n",
    "        self.b = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X shape: (batch_size, n_inputs)\n",
    "        Z = np.dot(X, self.W) + self.b\n",
    "        return Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fbd89c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z shape: (2, 4)\n",
      "[[-0.04037728 -0.04297277 -0.00931055  0.0064748 ]\n",
      " [ 0.02036559 -0.0005658  -0.00338519 -0.01383834]]\n"
     ]
    }
   ],
   "source": [
    "layer = DenseLayer(n_inputs=3, n_neurons=4)\n",
    "\n",
    "X = np.array([\n",
    "    [1.0, 2.0, -1.0],\n",
    "    [0.5, -0.2, 3.0]\n",
    "])\n",
    "\n",
    "Z = layer.forward(X)\n",
    "print(\"Z shape:\", Z.shape)\n",
    "print(Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1b6da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, Z):\n",
    "        return np.maximum(0, Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10cb5c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.0064748 ]\n",
      " [0.02036559 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "relu_layer = ReLU()\n",
    "A = relu_layer.forward(Z)\n",
    "print(A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8214d501",
   "metadata": {},
   "source": [
    "Layer + Activation Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1350bc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final logits: [[-6.72164687e-06  1.91492196e-04]]\n"
     ]
    }
   ],
   "source": [
    "dense1 = DenseLayer(3, 5)\n",
    "act1 = ReLU()\n",
    "\n",
    "dense2 = DenseLayer(5, 2)   # output layer (2 classes example)\n",
    "\n",
    "X = np.array([[1.0, 2.0, -1.0]])\n",
    "\n",
    "Z1 = dense1.forward(X)\n",
    "A1 = act1.forward(Z1)\n",
    "\n",
    "Z2 = dense2.forward(A1)\n",
    "print(\"Final logits:\", Z2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deae0ed",
   "metadata": {},
   "source": [
    "backdrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "979fdd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.W = np.random.randn(n_inputs, n_neurons) * 0.01\n",
    "        self.b = np.zeros((1, n_neurons))\n",
    "        self.X = None  # cache for backward\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        Z = np.dot(X, self.W) + self.b\n",
    "        return Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        # dZ shape: (batch_size, n_neurons)\n",
    "        m = self.X.shape[0]\n",
    "\n",
    "        dW = np.dot(self.X.T, dZ) / m\n",
    "        db = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "        dX = np.dot(dZ, self.W.T)\n",
    "\n",
    "        return dX, dW, db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7100cab6",
   "metadata": {},
   "source": [
    "backprop for relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d2be834",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.Z = None\n",
    "\n",
    "    def forward(self, Z):\n",
    "        self.Z = Z\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def backward(self, dA):\n",
    "        dZ = dA * (self.Z > 0)\n",
    "        return dZ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f24140",
   "metadata": {},
   "source": [
    "2 layer Neural nertowrk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ee9d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_pred, y_true):\n",
    "    return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "def mse_grad(y_pred, y_true):\n",
    "    m = y_true.shape[0]\n",
    "    return (2.0 * (y_pred - y_true)) / m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fe3730",
   "metadata": {},
   "source": [
    "training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1eaf03e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 13.3410\n",
      "Epoch 20, Loss: 13.3395\n",
      "Epoch 40, Loss: 13.3380\n",
      "Epoch 60, Loss: 13.3365\n",
      "Epoch 80, Loss: 13.3350\n",
      "Epoch 100, Loss: 13.3335\n",
      "Epoch 120, Loss: 13.3321\n",
      "Epoch 140, Loss: 13.3307\n",
      "Epoch 160, Loss: 13.3293\n",
      "Epoch 180, Loss: 13.3279\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Dummy dataset: y = 3x1 - 2x2 + noise\n",
    "m = 200\n",
    "X = np.random.randn(m, 2)\n",
    "y = (3 * X[:, 0] - 2 * X[:, 1] + 0.1 * np.random.randn(m)).reshape(-1, 1)\n",
    "\n",
    "# Model\n",
    "dense1 = DenseLayer(n_inputs=2, n_neurons=16)\n",
    "relu1 = ReLU()\n",
    "dense2 = DenseLayer(n_inputs=16, n_neurons=1)\n",
    "\n",
    "lr = 0.05\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # -------- forward ----------\n",
    "    Z1 = dense1.forward(X)\n",
    "    A1 = relu1.forward(Z1)\n",
    "    y_pred = dense2.forward(A1)\n",
    "\n",
    "    # -------- loss ----------\n",
    "    loss = mse_loss(y_pred, y)\n",
    "\n",
    "    # -------- backward ----------\n",
    "    dY = mse_grad(y_pred, y)          # dLoss/dOutput\n",
    "    dA1, dW2, db2 = dense2.backward(dY)\n",
    "    dZ1 = relu1.backward(dA1)\n",
    "    dX, dW1, db1 = dense1.backward(dZ1)\n",
    "\n",
    "    # -------- update ----------\n",
    "    dense2.W -= lr * dW2\n",
    "    dense2.b -= lr * db2\n",
    "\n",
    "    dense1.W -= lr * dW1\n",
    "    dense1.b -= lr * db1\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8769615",
   "metadata": {},
   "source": [
    "softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5885e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    Z_shift = Z - np.max(Z, axis=1, keepdims=True)  # stability\n",
    "    exp = np.exp(Z_shift)\n",
    "    return exp / np.sum(exp, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a9337a",
   "metadata": {},
   "source": [
    "cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd77cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(Y_pred, Y_true):\n",
    "    eps = 1e-12\n",
    "    Y_pred = np.clip(Y_pred, eps, 1.0 - eps)\n",
    "    return -np.mean(np.sum(Y_true * np.log(Y_pred), axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d57afd4",
   "metadata": {},
   "source": [
    "Softmax + Cross Entropy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060e8623",
   "metadata": {},
   "source": [
    "one-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1499279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y, num_classes):\n",
    "    Y = np.zeros((y.shape[0], num_classes))\n",
    "    Y[np.arange(y.shape[0]), y] = 1\n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3eb52793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(Y_pred_probs, y_true_labels):\n",
    "    preds = np.argmax(Y_pred_probs, axis=1)\n",
    "    return np.mean(preds == y_true_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf20f86",
   "metadata": {},
   "source": [
    "Full Neural Network (Dense + ReLU + Dense + Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55897503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.W = np.random.randn(n_inputs, n_neurons) * 0.01\n",
    "        self.b = np.zeros((1, n_neurons))\n",
    "        self.X = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        return np.dot(X, self.W) + self.b\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        m = self.X.shape[0]\n",
    "        dW = np.dot(self.X.T, dZ) / m\n",
    "        db = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "        dX = np.dot(dZ, self.W.T)\n",
    "        return dX, dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e2d592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.Z = None\n",
    "\n",
    "    def forward(self, Z):\n",
    "        self.Z = Z\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def backward(self, dA):\n",
    "        return dA * (self.Z > 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a426761f",
   "metadata": {},
   "source": [
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b30d5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_blobs(n_samples_per_class=200, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    centers = [(-2, 0), (2, 0), (0, 2.5)]\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        Xc = np.random.randn(n_samples_per_class, 2) * 0.7 + np.array(c)\n",
    "        yc = np.full(n_samples_per_class, i)\n",
    "        X_list.append(Xc)\n",
    "        y_list.append(yc)\n",
    "\n",
    "    X = np.vstack(X_list)\n",
    "    y = np.concatenate(y_list)\n",
    "\n",
    "    idx = np.random.permutation(X.shape[0])\n",
    "    return X[idx], y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01a4665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(X, y, batch_size=32, shuffle=True):\n",
    "    m = X.shape[0]\n",
    "    idx = np.arange(m)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "\n",
    "    for start in range(0, m, batch_size):\n",
    "        batch_idx = idx[start:start + batch_size]\n",
    "        yield X[batch_idx], y[batch_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff17c298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | Loss 1.0993 | Val Acc 0.0133\n",
      "Epoch  20 | Loss 1.0985 | Val Acc 0.3133\n",
      "Epoch  40 | Loss 1.0975 | Val Acc 0.3133\n",
      "Epoch  60 | Loss 1.0964 | Val Acc 0.3133\n",
      "Epoch  80 | Loss 1.0950 | Val Acc 0.3200\n",
      "Epoch 100 | Loss 1.0930 | Val Acc 0.7267\n",
      "Epoch 120 | Loss 1.0901 | Val Acc 0.9200\n",
      "Epoch 140 | Loss 1.0854 | Val Acc 0.9200\n",
      "Epoch 160 | Loss 1.0789 | Val Acc 0.9667\n",
      "Epoch 180 | Loss 1.0690 | Val Acc 0.9733\n",
      "Epoch 200 | Loss 1.0549 | Val Acc 0.9733\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def softmax(Z):\n",
    "    Z_shift = Z - np.max(Z, axis=1, keepdims=True)\n",
    "    exp = np.exp(Z_shift)\n",
    "    return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(Y_pred, Y_true):\n",
    "    eps = 1e-12\n",
    "    Y_pred = np.clip(Y_pred, eps, 1.0 - eps)\n",
    "    return -np.mean(np.sum(Y_true * np.log(Y_pred), axis=1))\n",
    "\n",
    "def one_hot(y, num_classes):\n",
    "    Y = np.zeros((y.shape[0], num_classes))\n",
    "    Y[np.arange(y.shape[0]), y] = 1\n",
    "    return Y\n",
    "\n",
    "def accuracy(Y_pred_probs, y_true_labels):\n",
    "    preds = np.argmax(Y_pred_probs, axis=1)\n",
    "    return np.mean(preds == y_true_labels)\n",
    "\n",
    "def iterate_minibatches(X, y, batch_size=32, shuffle=True):\n",
    "    m = X.shape[0]\n",
    "    idx = np.arange(m)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    for start in range(0, m, batch_size):\n",
    "        batch_idx = idx[start:start + batch_size]\n",
    "        yield X[batch_idx], y[batch_idx]\n",
    "\n",
    "def make_blobs(n_samples_per_class=200, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    centers = [(-2, 0), (2, 0), (0, 2.5)]\n",
    "    X_list, y_list = [], []\n",
    "    for i, c in enumerate(centers):\n",
    "        Xc = np.random.randn(n_samples_per_class, 2) * 0.7 + np.array(c)\n",
    "        yc = np.full(n_samples_per_class, i)\n",
    "        X_list.append(Xc)\n",
    "        y_list.append(yc)\n",
    "    X = np.vstack(X_list)\n",
    "    y = np.concatenate(y_list)\n",
    "    idx = np.random.permutation(X.shape[0])\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "# ---------- layers ----------\n",
    "class DenseLayer:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.W = np.random.randn(n_inputs, n_neurons) * 0.01\n",
    "        self.b = np.zeros((1, n_neurons))\n",
    "        self.X = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        return np.dot(X, self.W) + self.b\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        m = self.X.shape[0]\n",
    "        dW = np.dot(self.X.T, dZ) / m\n",
    "        db = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "        dX = np.dot(dZ, self.W.T)\n",
    "        return dX, dW, db\n",
    "\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.Z = None\n",
    "\n",
    "    def forward(self, Z):\n",
    "        self.Z = Z\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def backward(self, dA):\n",
    "        return dA * (self.Z > 0)\n",
    "\n",
    "# ---------- training ----------\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data\n",
    "X, y = make_blobs(n_samples_per_class=250, seed=1)\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# Train/Val split\n",
    "split = int(0.8 * X.shape[0])\n",
    "X_train, y_train = X[:split], y[:split]\n",
    "X_val, y_val = X[split:], y[split:]\n",
    "\n",
    "Y_train = one_hot(y_train, num_classes)\n",
    "Y_val = one_hot(y_val, num_classes)\n",
    "\n",
    "# Model: 2 -> 32 -> 3\n",
    "dense1 = DenseLayer(2, 32)\n",
    "relu1 = ReLU()\n",
    "dense2 = DenseLayer(32, num_classes)\n",
    "\n",
    "lr = 0.1\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_loss = 0.0\n",
    "    nb = 0\n",
    "\n",
    "    for Xb, yb in iterate_minibatches(X_train, y_train, batch_size=batch_size, shuffle=True):\n",
    "        Yb = one_hot(yb, num_classes)\n",
    "\n",
    "        # Forward\n",
    "        Z1 = dense1.forward(Xb)\n",
    "        A1 = relu1.forward(Z1)\n",
    "        Z2 = dense2.forward(A1)          # logits\n",
    "        Y_pred = softmax(Z2)             # probabilities\n",
    "\n",
    "        # Loss\n",
    "        loss = cross_entropy_loss(Y_pred, Yb)\n",
    "        epoch_loss += loss\n",
    "        nb += 1\n",
    "\n",
    "        # Backward (Softmax + CE gradient)\n",
    "        m = Xb.shape[0]\n",
    "        dZ2 = (Y_pred - Yb) / m\n",
    "\n",
    "        dA1, dW2, db2 = dense2.backward(dZ2)\n",
    "        dZ1 = relu1.backward(dA1)\n",
    "        _, dW1, db1 = dense1.backward(dZ1)\n",
    "\n",
    "        # Update\n",
    "        dense2.W -= lr * dW2\n",
    "        dense2.b -= lr * db2\n",
    "        dense1.W -= lr * dW1\n",
    "        dense1.b -= lr * db1\n",
    "\n",
    "    # Validate\n",
    "    Z1_val = dense1.forward(X_val)\n",
    "    A1_val = relu1.forward(Z1_val)\n",
    "    Z2_val = dense2.forward(A1_val)\n",
    "    Y_val_pred = softmax(Z2_val)\n",
    "\n",
    "    val_acc = accuracy(Y_val_pred, y_val)\n",
    "\n",
    "    if epoch % 20 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:3d} | Loss {epoch_loss/nb:.4f} | Val Acc {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "357b78d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_true, y_pred, num_classes):\n",
    "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        cm[t, p] += 1\n",
    "    return cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3009af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_f1(cm):\n",
    "    num_classes = cm.shape[0]\n",
    "    precision = np.zeros(num_classes)\n",
    "    recall = np.zeros(num_classes)\n",
    "    f1 = np.zeros(num_classes)\n",
    "\n",
    "    for k in range(num_classes):\n",
    "        TP = cm[k, k]\n",
    "        FP = np.sum(cm[:, k]) - TP\n",
    "        FN = np.sum(cm[k, :]) - TP\n",
    "\n",
    "        precision[k] = TP / (TP + FP + 1e-12)\n",
    "        recall[k] = TP / (TP + FN + 1e-12)\n",
    "        f1[k] = 2 * precision[k] * recall[k] / (precision[k] + recall[k] + 1e-12)\n",
    "\n",
    "    return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "922decba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_weighted_scores(cm, precision, recall, f1):\n",
    "    support = np.sum(cm, axis=1)  # samples per true class\n",
    "    total = np.sum(support)\n",
    "\n",
    "    macro_p = np.mean(precision)\n",
    "    macro_r = np.mean(recall)\n",
    "    macro_f1 = np.mean(f1)\n",
    "\n",
    "    weighted_p = np.sum(precision * support) / (total + 1e-12)\n",
    "    weighted_r = np.sum(recall * support) / (total + 1e-12)\n",
    "    weighted_f1 = np.sum(f1 * support) / (total + 1e-12)\n",
    "\n",
    "    return {\n",
    "        \"macro_precision\": macro_p,\n",
    "        \"macro_recall\": macro_r,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"weighted_precision\": weighted_p,\n",
    "        \"weighted_recall\": weighted_r,\n",
    "        \"weighted_f1\": weighted_f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5ddc2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(Y_pred_probs, y_true, num_classes):\n",
    "    Y_true_oh = np.zeros((y_true.shape[0], num_classes))\n",
    "    Y_true_oh[np.arange(y_true.shape[0]), y_true] = 1\n",
    "    return cross_entropy_loss(Y_pred_probs, Y_true_oh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c75a883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_accuracy(Y_pred_probs, y_true, k=2):\n",
    "    top_k = np.argsort(Y_pred_probs, axis=1)[:, -k:]  # top k indices\n",
    "    correct = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        if y_true[i] in top_k[i]:\n",
    "            correct += 1\n",
    "    return correct / y_true.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "713188c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_confidence(Y_pred_probs):\n",
    "    return np.mean(np.max(Y_pred_probs, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "186829b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_calibration_error(Y_pred_probs, y_true, n_bins=10):\n",
    "    confidences = np.max(Y_pred_probs, axis=1)\n",
    "    predictions = np.argmax(Y_pred_probs, axis=1)\n",
    "    accuracies = (predictions == y_true).astype(float)\n",
    "\n",
    "    ece = 0.0\n",
    "    bin_edges = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "\n",
    "    for i in range(n_bins):\n",
    "        low, high = bin_edges[i], bin_edges[i + 1]\n",
    "        mask = (confidences > low) & (confidences <= high)\n",
    "\n",
    "        if np.sum(mask) == 0:\n",
    "            continue\n",
    "\n",
    "        bin_acc = np.mean(accuracies[mask])\n",
    "        bin_conf = np.mean(confidences[mask])\n",
    "        ece += (np.sum(mask) / len(y_true)) * np.abs(bin_acc - bin_conf)\n",
    "\n",
    "    return ece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86d29cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(Y_pred_probs, y_true, num_classes):\n",
    "    y_pred = np.argmax(Y_pred_probs, axis=1)\n",
    "\n",
    "    acc = np.mean(y_pred == y_true)\n",
    "    cm = confusion_matrix(y_true, y_pred, num_classes)\n",
    "\n",
    "    precision, recall, f1 = precision_recall_f1(cm)\n",
    "    avg_scores = macro_weighted_scores(cm, precision, recall, f1)\n",
    "\n",
    "    loss = log_loss(Y_pred_probs, y_true, num_classes)\n",
    "    top2 = top_k_accuracy(Y_pred_probs, y_true, k=2)\n",
    "\n",
    "    conf = avg_confidence(Y_pred_probs)\n",
    "    ece = expected_calibration_error(Y_pred_probs, y_true, n_bins=10)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"log_loss\": loss,\n",
    "        \"top2_accuracy\": top2,\n",
    "        \"avg_confidence\": conf,\n",
    "        \"ece\": ece,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"precision_per_class\": precision,\n",
    "        \"recall_per_class\": recall,\n",
    "        \"f1_per_class\": f1,\n",
    "        **avg_scores\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa99c65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9733333333333334\n",
      "Log Loss: 1.051850390446666\n",
      "Top-2 Acc: 1.0\n",
      "Avg Confidence: 0.34944069118327026\n",
      "ECE: 0.6238926421500631\n",
      "\n",
      "Confusion Matrix:\n",
      " [[47  0  0]\n",
      " [ 1 49  2]\n",
      " [ 1  0 50]]\n",
      "\n",
      "F1 per class: [0.97916667 0.97029703 0.97087379]\n",
      "Macro F1: 0.9734458275919486\n",
      "Weighted F1: 0.973272279897367\n"
     ]
    }
   ],
   "source": [
    "# Y_val_pred_probs = softmax(Z2_val)\n",
    "results = evaluate_model(Y_val_pred, y_val, num_classes)\n",
    "\n",
    "print(\"Accuracy:\", results[\"accuracy\"])\n",
    "print(\"Log Loss:\", results[\"log_loss\"])\n",
    "print(\"Top-2 Acc:\", results[\"top2_accuracy\"])\n",
    "print(\"Avg Confidence:\", results[\"avg_confidence\"])\n",
    "print(\"ECE:\", results[\"ece\"])\n",
    "\n",
    "print(\"\\nConfusion Matrix:\\n\", results[\"confusion_matrix\"])\n",
    "print(\"\\nF1 per class:\", results[\"f1_per_class\"])\n",
    "print(\"Macro F1:\", results[\"macro_f1\"])\n",
    "print(\"Weighted F1:\", results[\"weighted_f1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08e589c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
